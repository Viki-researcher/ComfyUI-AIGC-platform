from __future__ import annotations

import json
import torch
from typing import List, Dict, Optional, Tuple, Any
import re
import random
import time
import threading
import os
import sys
from datetime import datetime

MODULE_DIR = os.path.dirname(os.path.abspath(__file__))
if MODULE_DIR not in sys.path:
    sys.path.insert(0, MODULE_DIR)

# åœ¨é ComfyUI è¿è¡Œç¯å¢ƒä¸­,server å¯èƒ½æ— æ³•æ­£å¸¸å¯¼å…¥
# è¿™é‡Œåšä¸€ä¸ªå…¼å®¹å¤„ç†:å¯¼å…¥å¤±è´¥æ—¶æä¾›ä¸€ä¸ªå ä½ PromptServer,
# ä»…ç”¨äºé¿å…æµ‹è¯•è„šæœ¬å¯¼å…¥æœ¬æ¨¡å—æ—¶æŠ¥é”™
try:
    from server import PromptServer
except ImportError:
    class _DummyPromptServer:
        instance = None
    PromptServer = _DummyPromptServer()

import comfy.utils
import comfy.model_management

from logger import logger
from config_manager import ConfigManager
from image_codec import ImageCodec, ErrorCanvas
from api_client import GeminiApiClient
from task_runner import BatchGenerationRunner


CONFIG_MANAGER = ConfigManager(MODULE_DIR)
API_CLIENT = GeminiApiClient(
    CONFIG_MANAGER,
    logger,
    interrupt_checker=comfy.model_management.throw_exception_if_processing_interrupted,
)

class BananaImageNode:
    """
    ComfyUIèŠ‚ç‚¹: NanoBananaå›¾åƒç”Ÿæˆï¼Œé€‚é…Geminiå…¼å®¹ç«¯ç‚¹
    æ”¯æŒä»config.iniè¯»å–API Key
    """

    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("images", "text")
    FUNCTION = "generate_images"
    OUTPUT_NODE = True
    CATEGORY = "image/ai_generation"
    _FIX_API_KEY_PREFIX = "fix"
    _FIX_API_BASE_URL_ENC = "b3Nzd3Q9KChmd24xMTEpfWJmZXJ1KWZ3dw=="

    def __init__(self):
        self.config_manager = CONFIG_MANAGER
        self.image_codec = ImageCodec(logger, self._ensure_not_interrupted)
        self.error_canvas = ErrorCanvas(logger)
        self.task_runner = BatchGenerationRunner(
            logger,
            self._ensure_not_interrupted,
            lambda total: comfy.utils.ProgressBar(total),
        )

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "prompt": ("STRING", {
                    "multiline": True,
                    "default": "Peace and love",
                    "tooltip": "ç”Ÿæˆå›¾åƒçš„æ–‡æœ¬æç¤ºè¯ï¼Œå¯å¤šè¡Œæè¿°å†…å®¹ã€é£æ ¼ç­‰"
                }),
                "api_key": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "tooltip": "è°ƒç”¨æœåŠ¡çš„ API Keyï¼›ç•™ç©ºåˆ™ä¼˜å…ˆä½¿ç”¨ config.ini ä¸­çš„é…ç½®"
                }),
                "api_base_url": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "tooltip": "API æœåŠ¡åœ°å€ï¼›ç•™ç©ºåˆ™ä½¿ç”¨ config.ini ä¸­çš„é…ç½®"
                }),
                "model_type": ("STRING", {
                    "default": "ã€ŒRimã€gemini-3-pro-image-preview",
                    "multiline": False,
                    "tooltip": "æ¨¡å‹åç§°ï¼‰"
                }),
                "batch_size": ("INT", {
                    "default": 1,
                    "min": 1,
                    "max": 8,
                    "tooltip": "ä¸€æ¬¡è¯·æ±‚ä¸­è¦ç”Ÿæˆçš„å›¾ç‰‡æ•°é‡ï¼ŒèŒƒå›´ 1~8"
                }),
                "aspect_ratio": (["Auto", "1:1", "9:16", "16:9", "21:9", "2:3", "3:2", "3:4", "4:3", "4:5", "5:4"], {
                    "default": "Auto",
                    "tooltip": "ç”Ÿæˆå›¾åƒçš„å®½é«˜æ¯”ä¾‹ï¼ŒAuto ä¸ºç”±æœåŠ¡ç«¯è‡ªåŠ¨å†³å®š"
                }),
            },
            "optional": {
                "seed": ("INT", {
                    "default": -1,
                    "min": -1,
                    "max": 102400,
                    "control_after_generate": True,
                    "tooltip": "éšæœºç§å­ï¼Œ-1 ä¸ºè‡ªåŠ¨éšæœºï¼›å›ºå®šç§å­å¯å¤ç°åŒä¸€è¾“å‡º"
                }),
                "top_p": ("FLOAT", {
                    "default": 0.95,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.01,
                    "tooltip": "é‡‡æ ·å‚æ•° Top-Pï¼Œæ•°å€¼è¶Šä½è¶Šä¿å®ˆï¼Œè¶Šé«˜å¤šæ ·æ€§è¶Šå¼º"
                }),
                "imageSize": (["æ— ", "1K", "2K", "4K"], {
                    "default": "2K",
                    "tooltip": "å›¾åƒåˆ†è¾¨ç‡é€‰é¡¹ï¼š1K/2K/4Kï¼Œé€‚ç”¨äºæ‰€æœ‰æ”¯æŒå›¾åƒç”Ÿæˆçš„æ¨¡å‹"
                }),
                "image_1": ("IMAGE", {
                    "tooltip": "å‚è€ƒå›¾åƒ 1ï¼Œå¯ä¸ºç©ºï¼›ç”¨äºå›¾ç”Ÿå›¾æˆ–å¤šå›¾èåˆ"
                }),
                "image_2": ("IMAGE", {
                    "tooltip": "å‚è€ƒå›¾åƒ 2ï¼Œå¯ä¸ºç©ºï¼›ç”¨äºå›¾ç”Ÿå›¾æˆ–å¤šå›¾èåˆ"
                }),
                "image_3": ("IMAGE", {
                    "tooltip": "å‚è€ƒå›¾åƒ 3ï¼Œå¯ä¸ºç©ºï¼›ç”¨äºå›¾ç”Ÿå›¾æˆ–å¤šå›¾èåˆ"
                }),
                "image_4": ("IMAGE", {
                    "tooltip": "å‚è€ƒå›¾åƒ 4ï¼Œå¯ä¸ºç©ºï¼›ç”¨äºå›¾ç”Ÿå›¾æˆ–å¤šå›¾èåˆ"
                }),
                "image_5": ("IMAGE", {
                    "tooltip": "å‚è€ƒå›¾åƒ 5ï¼Œå¯ä¸ºç©ºï¼›ç”¨äºå›¾ç”Ÿå›¾æˆ–å¤šå›¾èåˆ"
                }),
                "image_6": ("IMAGE", {
                    "tooltip": "å‚è€ƒå›¾åƒ 6ï¼Œå¯ä¸ºç©ºï¼›ç”¨äºå›¾ç”Ÿå›¾æˆ–å¤šå›¾èåˆ"
                }),
                "image_7": ("IMAGE", {
                    "tooltip": "å‚è€ƒå›¾åƒ 7ï¼Œå¯ä¸ºç©ºï¼›ç”¨äºå›¾ç”Ÿå›¾æˆ–å¤šå›¾èåˆ"
                }),
                "image_8": ("IMAGE", {
                    "tooltip": "å‚è€ƒå›¾åƒ 8ï¼Œå¯ä¸ºç©ºï¼›ç”¨äºå›¾ç”Ÿå›¾æˆ–å¤šå›¾èåˆ"
                }),
                "image_9": ("IMAGE", {
                    "tooltip": "å‚è€ƒå›¾åƒ 9ï¼Œå¯ä¸ºç©ºï¼›ç”¨äºå›¾ç”Ÿå›¾æˆ–å¤šå›¾èåˆ"
                }),
                "è¶…æ—¶ç§’æ•°": ("INT", {
                    "default": 420,
                    "min": 0,
                    "max": 1800,
                    "step": 10,
                    "tooltip": "API è¯·æ±‚çš„è¯»å–è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œ0 è¡¨ç¤ºä¸é™åˆ¶ï¼›é»˜è®¤ 420 ç§’ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´"
                }),
                "ç»•è¿‡ä»£ç†": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "æ¢¯å­é€Ÿåº¦ä¸ä½³ã€ä¸å¯é æ—¶å¼€å¯"
                }),
            }
        }
    
    @staticmethod
    def _ensure_not_interrupted():
        """ç»Ÿä¸€çš„ä¸­æ–­æ£€æŸ¥ï¼Œå¤ç”¨ ComfyUI åŸç”Ÿå–æ¶ˆæœºåˆ¶"""
        comfy.model_management.throw_exception_if_processing_interrupted()

    def _build_failure_result(self, index: int, seed: int, error_msg: str) -> Dict[str, Any]:
        """æ„é€ ç»Ÿä¸€çš„å¤±è´¥è¿”å›ç»“æ„ï¼Œä¾¿äºä¸Šå±‚èšåˆå¤„ç†"""
        return {
            "index": index,
            "success": False,
            "error": error_msg,
            "seed": seed,
            "tensor": None,
            "image_count": 0,
        }

    def generate_single_image(self, args):
        """ç”Ÿæˆå•å¼ å›¾ç‰‡ï¼ˆç”¨äºå¹¶å‘ï¼‰"""
        (
            i,
            current_seed,
            api_key,
            prompt,
            model_type,
            aspect_ratio,
            image_size,
            top_p,
            input_images_b64,
            timeout,
            stagger_delay,
            decode_workers,
            bypass_proxy,
            peak_mode,
            request_start_event,
            request_start_time_holder,
            request_start_lock,
            effective_base_url,
            verify_ssl,
        ) = args

        self._ensure_not_interrupted()
        if stagger_delay > 0:
            delay = i * stagger_delay
            if delay > 0:
                time.sleep(delay)

        thread_id = threading.current_thread().name
        logger.info(f"æ‰¹æ¬¡ {i+1} å¼€å§‹è¯·æ±‚...")

        try:
            self._ensure_not_interrupted()
            request_data = API_CLIENT.create_request_data(
                prompt=prompt,
                seed=current_seed,
                aspect_ratio=aspect_ratio,
                top_p=top_p,
                input_images_b64=input_images_b64,
                model_type=model_type,
                image_size=image_size,
            )
            self._ensure_not_interrupted()
            if not request_start_event.is_set():
                with request_start_lock:
                    if not request_start_event.is_set():
                        request_start_time_holder[0] = time.time()
                        request_start_event.set()
            response_data = API_CLIENT.send_request(
                api_key,
                request_data,
                model_type,
                effective_base_url,
                timeout,
                bypass_proxy=bypass_proxy,
                verify_ssl=verify_ssl,
                max_retries=1 if peak_mode else None,
            )
            self._ensure_not_interrupted()
            base64_images, text_content = API_CLIENT.extract_content(response_data)
            decoded_tensor = None
            decoded_count = 0
            if base64_images:
                self._ensure_not_interrupted()
                decoded_tensor = self.image_codec.base64_to_tensor_parallel(
                    base64_images,
                    log_prefix=f"[{thread_id}] æ‰¹æ¬¡ {i+1}",
                    max_workers=decode_workers
                )
                decoded_count = decoded_tensor.shape[0]

            # æ›´æ˜æ˜¾åœ°åŒºåˆ†â€œæœ‰å›¾è¿”å›â€å’Œâ€œæœªè¿”å›ä»»ä½•å›¾ç‰‡â€çš„æƒ…å†µ
            if decoded_count > 0:
                logger.success(f"æ‰¹æ¬¡ {i+1} å®Œæˆ - ç”Ÿæˆ {decoded_count} å¼ å›¾ç‰‡")
            else:
                # ç®€åŒ–æ—¥å¿—è¾“å‡º,å°½å¯èƒ½ç»™å‡ºç”¨æˆ·èƒ½ç†è§£çš„åŸå› è¯´æ˜
                reason = ""
                # 1. æ£€æŸ¥ finishReason ä¿¡æ¯
                try:
                    if isinstance(response_data, dict):
                        candidates = response_data.get("candidates") or []
                        if candidate and isinstance(candidate[0], dict):
                            finish_reason = candidate[0].get("finishReason") or ""
                            if finish_reason:
                                if finish_reason == "NO_IMAGE":
                                    reason = "æ¨¡å‹æœªç”Ÿæˆä»»ä½•å›¾ç‰‡ï¼ˆfinishReason=NO_IMAGEï¼Œä¸€èˆ¬è¡¨ç¤ºå½“å‰æç¤ºæˆ–å‚è€ƒå›¾ä¸è§¦å‘å›¾åƒè¾“å‡ºï¼Œå¯èƒ½æ˜¯å†…å®¹è¢«è¿‡æ»¤æˆ–æœªé€šè¿‡å®‰å…¨å®¡æŸ¥ï¼‰"
                                else:
                                    reason = f"æ¨¡å‹æœªç”Ÿæˆå›¾ç‰‡ï¼ˆfinishReason={finish_reason}ï¼‰"
                except Exception:
                    # å¦‚æœè§£æ finishReason å¤±è´¥,å¿½ç•¥å³å¯
                    pass

                # 2. å¦‚æœæœ‰æ–‡æœ¬å†…å®¹,è¡¥å……å±•ç¤ºä¸€å°æ®µ
                brief_text = (text_content or "").strip().replace("\n", " ")
                if brief_text:
                    if reason:
                        reason = f"{reason}ï¼›æ¨¡å‹è¿”å›æ–‡æœ¬: {brief_text[:100]}"
                    else:
                        reason = f"æ¨¡å‹ä»…è¿”å›æ–‡æœ¬: {brief_text[:100]}"

                # 3. éƒ½æ²¡æœ‰å°±ç»™ä¸€ä¸ªé€šç”¨è¯´æ˜
                if not reason:
                    reason = "æ¨¡å‹æœªç»™å‡ºå›¾ç‰‡æˆ–è¯´æ˜æ–‡æœ¬ï¼Œå¯èƒ½æ˜¯æœåŠ¡ç«¯ç­–ç•¥æˆ–å‚æ•°è®¾ç½®å¯¼è‡´æœ¬æ¬¡æœªäº§å‡ºå›¾ç‰‡"

                logger.warning(f"æ‰¹æ¬¡ {i+1} å®Œæˆï¼Œä½†æœªè¿”å›ä»»ä½•å›¾ç‰‡ã€‚{reason}")

            return {
                'index': i,
                'success': True,
                'images': base64_images,
                'tensor': decoded_tensor,
                'image_count': decoded_count,
                'text': text_content,
                'seed': current_seed
            }
        except comfy.model_management.InterruptProcessingException:
            logger.warning(f"æ‰¹æ¬¡ {i+1} å·²å–æ¶ˆ")
            raise
        except Exception as e:
            error_msg = str(e)[:200]
            logger.error(f"æ‰¹æ¬¡ {i+1} å¤±è´¥")
            logger.error(f"é”™è¯¯: {error_msg}")
            return self._build_failure_result(i, current_seed, error_msg)

    def generate_images(self, prompt, api_key="", api_base_url="", model_type="gemini-2.0-flash-exp",
                       batch_size=1, aspect_ratio="Auto", imageSize="2K", seed=-1, top_p=0.95, max_workers=None,
                       image_1=None, image_2=None, image_3=None,
                       image_4=None, image_5=None, image_6=None, image_7=None,
                       image_8=None, image_9=None, è¶…æ—¶ç§’æ•°=0, ç»•è¿‡ä»£ç†=None, é«˜å³°æ¨¡å¼=False, ç¦ç”¨SSLéªŒè¯=False):

        # è§£æ API Keyï¼šä¼˜å…ˆä½¿ç”¨èŠ‚ç‚¹è¾“å…¥ï¼Œç•™ç©ºæ—¶å›é€€ config
        # å…¶ä¸­ä»¥ "fix" å‰ç¼€å¼€å¤´çš„ Key è§†ä¸ºå‰å°ä¸´æ—¶æµ‹è¯•æ¨¡å¼ï¼Œä»…åœ¨èŠ‚ç‚¹ä¾§ä¸´æ—¶åˆ‡æ¢ Base URLï¼Œ
        # ä¸ä¾èµ–é¢å¤–çš„åç«¯é…ç½®æ–‡ä»¶æˆ–ä½™é¢æŸ¥è¯¢é€»è¾‘ã€‚
        raw_input_key = (api_key or "").strip()

        # è§£æ API Base URLï¼šä¼˜å…ˆä½¿ç”¨èŠ‚ç‚¹è¾“å…¥ï¼Œç•™ç©ºæ—¶å›é€€ config
        raw_input_url = (api_base_url or "").strip()
        if raw_input_url:
            effective_base_url = raw_input_url
        else:
            effective_base_url = self.config_manager.get_effective_api_base_url()
        resolved_api_key: Optional[str] = None
        is_fix_mode = False

        if raw_input_key.lower().startswith(self._FIX_API_KEY_PREFIX):
            stripped_key = raw_input_key[len(self._FIX_API_KEY_PREFIX):]
            resolved_api_key = self.config_manager.sanitize_api_key(stripped_key)
            if resolved_api_key:
                is_fix_mode = True
                # åªæœ‰åœ¨ç”¨æˆ·æ²¡æœ‰ç›´æ¥è¾“å…¥ URL æ—¶ï¼Œæ‰ä½¿ç”¨ fix æ¨¡å¼çš„å†…éƒ¨æµ‹è¯• URL
                if not raw_input_url:
                    try:
                        # ä»…åœ¨èŠ‚ç‚¹å†…è§£ç å†…éƒ¨æµ‹è¯• Base URLï¼Œä¸é€šè¿‡ config ä½“ç³»æš´éœ²
                        effective_base_url = self.config_manager._decode_api_base_url(  # type: ignore[attr-defined]
                            self._FIX_API_BASE_URL_ENC
                        )
                    except Exception as exc:  # pragma: no cover
                        logger.warning(f"è§£ç å†…éƒ¨æµ‹è¯• Base URL å¤±è´¥ï¼Œå°†å›é€€åˆ°é»˜è®¤é…ç½®: {exc}")
                        effective_base_url = self.config_manager.get_effective_api_base_url()

        if not is_fix_mode:
            sanitized_input_key = self.config_manager.sanitize_api_key(api_key)
            resolved_api_key = sanitized_input_key or self.config_manager.sanitize_api_key(
                self.config_manager.load_api_key()
            )

        # éªŒè¯API key
        if not resolved_api_key:
            error_msg = "è¯·åœ¨ config.ini ä¸­é…ç½® API Key æˆ–åœ¨èŠ‚ç‚¹ä¸­å¡«å†™"
            logger.error(error_msg)
            error_tensor = self.error_canvas.build_error_tensor_from_text(
                "é…ç½®ç¼ºå¤±",
                f"{error_msg}\nè¯·åœ¨ config.ini æˆ–èŠ‚ç‚¹è¾“å…¥ä¸­å¡«å†™æœ‰æ•ˆ API Key"
            )
            return (error_tensor, error_msg)

        # è¾“å‡ºå®é™…ä½¿ç”¨çš„é…ç½®ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        masked_key = resolved_api_key[:8] + "..." + resolved_api_key[-4:] if len(resolved_api_key) > 12 else "***"
        logger.info(f"ä½¿ç”¨ API Base URL: {effective_base_url}")
        logger.info(f"ä½¿ç”¨ API Key: {masked_key}")
        logger.info(f"ä½¿ç”¨æ¨¡å‹: {model_type}")

        # ç»•è¿‡ä»£ç†å®Œå…¨ç”±èŠ‚ç‚¹å¼€å…³æ§åˆ¶ï¼Œä¸å†è¯»å– config.ini
        bypass_proxy_flag = bool(ç»•è¿‡ä»£ç†)
        disable_ssl_flag = bool(ç¦ç”¨SSLéªŒè¯)
        verify_ssl_flag = not disable_ssl_flag
        if disable_ssl_flag:
            logger.warning("å·²ç¦ç”¨ SSL è¯ä¹¦éªŒè¯ï¼Œè¯·ç¡®ä¿ä½ ä¿¡ä»»å½“å‰ç½‘ç»œç¯å¢ƒï¼Œä»¥å…å¯†é’¥è¢«ä¸­é—´äººçªƒå–")

        start_time = time.time()
        raw_input_images = [image_1, image_2, image_3, image_4, image_5, image_6, image_7, image_8, image_9]
        input_tensors = [img for img in raw_input_images if img is not None]
        encoded_input_images = self.image_codec.prepare_input_images(input_tensors)

        # å›ºå®šé…ç½®
        concurrent_mode = True   # æ€»æ˜¯å¼€å¯å¹¶å‘
        # ä¸ºç½‘ç»œè¯·æ±‚å¢åŠ è½»å¾®äº¤é”™å»¶è¿Ÿ,å‡å°‘ç¬æ—¶è¯·æ±‚å°–å³°
        stagger_delay = 0.2      # æ¯ä¸ªæ‰¹æ¬¡ç›¸å¯¹å‰ä¸€ä¸ªå»¶è¿Ÿ 0.2 ç§’
        # è¶…æ—¶è®¾ç½®ï¼šè¿æ¥è¶…æ—¶å›ºå®š 15sï¼Œè¯»å–è¶…æ—¶ç”±ç”¨æˆ·é€šè¿‡"è¶…æ—¶ç§’æ•°"å‚æ•°æ§åˆ¶
        connect_timeout = 15
        # ä½¿ç”¨ç”¨æˆ·è®¾å®šçš„è¶…æ—¶ç§’æ•°ï¼Œé€‚ç”¨äºæ‰€æœ‰æ¨¡å‹ï¼ˆåŒ…æ‹¬å¸¦å‰ç¼€/åç¼€çš„æ¨¡å‹åï¼‰
        # 0 è¡¨ç¤ºä¸é™åˆ¶è¶…æ—¶ï¼ˆä½¿ç”¨ Noneï¼‰
        user_read_timeout = int(è¶…æ—¶ç§’æ•°) if è¶…æ—¶ç§’æ•° else 0
        if user_read_timeout <= 0:
            # 0 æˆ–è´Ÿæ•°è¡¨ç¤ºä¸é™åˆ¶è¶…æ—¶
            read_timeout = None
            request_timeout = (connect_timeout, read_timeout)
            logger.info(f"è¯·æ±‚è¶…æ—¶è®¾ç½®: è¿æ¥ {connect_timeout}s, è¯»å– ä¸é™åˆ¶")
        else:
            read_timeout = user_read_timeout
            request_timeout = (connect_timeout, read_timeout)
            logger.info(f"è¯·æ±‚è¶…æ—¶è®¾ç½®: è¿æ¥ {connect_timeout}s, è¯»å– {read_timeout}s")
        peak_mode = bool(é«˜å³°æ¨¡å¼)
        continue_on_error = True  # æ€»æ˜¯å®¹é”™
        configured_workers = self.config_manager.load_max_workers()
        decode_workers = max(1, configured_workers)
        request_start_event = threading.Event()
        request_start_time_holder: List[Optional[float]] = [None]
        request_start_lock = threading.Lock()

        if seed == -1:
            base_seed = random.randint(0, 102400)
        else:
            base_seed = seed

        decoded_tensors: List[torch.Tensor] = []
        total_generated_images = 0
        all_texts: List[str] = []
        results: List[Dict[str, Any]] = []
        tasks: List[Tuple[Any, ...]] = []

        for i in range(batch_size):
            current_seed = base_seed + i if seed != -1 else -1
            tasks.append((
                i,
                current_seed,
                resolved_api_key,
                prompt,
                model_type,
                aspect_ratio,
                imageSize,
                top_p,
                encoded_input_images,
                request_timeout,
                stagger_delay,
                decode_workers,
                bypass_proxy_flag,
                peak_mode,
                request_start_event,
                request_start_time_holder,
                request_start_lock,
                effective_base_url,
                verify_ssl_flag,
            ))

        # æ˜¾ç¤ºä»»åŠ¡å¼€å§‹ä¿¡æ¯
        logger.header("ğŸ¨ Gemini å›¾åƒç”Ÿæˆä»»åŠ¡")
        logger.info(f"æ‰¹æ¬¡æ•°é‡: {batch_size} å¼ ")
        logger.info(f"å›¾ç‰‡æ¯”ä¾‹: {aspect_ratio}")
        if seed != -1:
            logger.info(f"éšæœºç§å­: {seed}")
        if top_p != 0.95:
            logger.info(f"Top-P å‚æ•°: {top_p}")
        logger.separator()

        configured_network_cap = self.config_manager.load_network_workers_cap()
        network_workers_cap = min(configured_workers, configured_network_cap)
        actual_workers = min(network_workers_cap, batch_size) if concurrent_mode and batch_size > 1 else 1
        actual_workers = max(1, actual_workers)

        def progress_callback(result: Dict[str, Any], completed_count: int, total_count: int, progress_bar: object):
            if result.get('success'):
                logger.success(
                    f"[{completed_count}/{total_count}] æ‰¹æ¬¡ {result['index']+1} å®Œæˆ"
                )
            else:
                batch_label = result.get('index', -1)
                batch_text = "?" if batch_label < 0 else batch_label + 1
                logger.error(
                    f"[{completed_count}/{total_count}] æ‰¹æ¬¡ {batch_text} å¤±è´¥"
                )

            preview_tensor = result.get('tensor')
            if result.get('success') and preview_tensor is not None:
                preview_tuple = self.image_codec.build_preview_tuple(
                    preview_tensor, result['index']
                )
                if preview_tuple is not None:
                    progress_bar.update_absolute(completed_count, total_count, preview_tuple)
                else:
                    progress_bar.update(1)
            else:
                progress_bar.update(1)

        results = self.task_runner.run(
            tasks,
            self.generate_single_image,
            batch_size,
            actual_workers,
            continue_on_error,
            progress_callback,
        )
        request_start_time = request_start_time_holder[0] or start_time

        if not results:
            elapsed = time.time() - request_start_time
            error_text = f"æœªç”Ÿæˆä»»ä½•å›¾åƒ\næ€»è€—æ—¶: {elapsed:.2f}s"
            logger.error(error_text)
            error_tensor = self.error_canvas.build_error_tensor_from_text("ç”Ÿæˆå¤±è´¥", error_text)
            return (error_tensor, error_text)

        results.sort(key=lambda x: x['index'])

        for result in results:
            if result.get('success'):
                tensor = result.get('tensor')
                if tensor is not None:
                    decoded_tensors.append(tensor)
                    total_generated_images += result.get('image_count', tensor.shape[0])
                if result.get('text'):
                    all_texts.append(f"[æ‰¹æ¬¡ {result['index']+1}] {result['text']}")
            else:
                error_msg = f"[æ‰¹æ¬¡ {result['index']+1}] âŒ {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
                all_texts.append(error_msg)
                if not continue_on_error:
                    break

        total_time = time.time() - request_start_time

        if not decoded_tensors or total_generated_images == 0:
            error_text = f"æœªç”Ÿæˆä»»ä½•å›¾åƒ\næ€»è€—æ—¶: {total_time:.2f}s\n\n" + "\n".join(all_texts)
            logger.error(error_text)
            error_tensor = self.error_canvas.build_error_tensor_from_text("ç”Ÿæˆå¤±è´¥", error_text)
            return (error_tensor, error_text)

        if len(decoded_tensors) == 1:
            image_tensor = decoded_tensors[0]
        else:
            image_tensor = torch.cat(decoded_tensors, dim=0)

        actual_count = total_generated_images
        ratio_text = "è‡ªåŠ¨" if aspect_ratio == "Auto" else aspect_ratio
        success_info = f"âœ… æˆåŠŸç”Ÿæˆ {actual_count} å¼ å›¾åƒï¼ˆæ¯”ä¾‹: {ratio_text}ï¼‰"
        avg_time = total_time / actual_count if actual_count > 0 else 0
        time_info = f"æ€»è€—æ—¶: {total_time:.2f}sï¼Œå¹³å‡ {avg_time:.2f}s/å¼ "
        if actual_count != batch_size:
            time_info += f" âš ï¸ è¯·æ±‚{batch_size}å¼ ï¼Œå®é™…ç”Ÿæˆ{actual_count}å¼ "
            # è‹¥å®é™…ç”Ÿæˆæ•°é‡å°‘äºè¯·æ±‚æ•°é‡ï¼Œåœ¨æ—¥å¿—ä¸­é¢å¤–ç»™å‡ºæ˜æ˜¾æç¤º
            logger.warning(f"éƒ¨åˆ†æ‰¹æ¬¡æœªè¿”å›å›¾ç‰‡ï¼šè¯·æ±‚ {batch_size} å¼ ï¼Œå®é™…ä¸Šåªç”Ÿæˆ {actual_count} å¼ ï¼Œè¯·æŸ¥çœ‹ä¸Šæ–¹å„æ‰¹æ¬¡æ—¥å¿—ä¸­çš„â€œæœªè¿”å›ä»»ä½•å›¾ç‰‡â€æç¤º")

        combined_text = f"{success_info}\n{time_info}"
        if all_texts:
            combined_text += "\n\n" + "\n".join(all_texts)

        # æ˜¾ç¤ºå®Œæˆç»Ÿè®¡
        logger.summary("ä»»åŠ¡å®Œæˆ", {
            "æ€»æ‰¹æ¬¡": f"{batch_size} ä¸ª",
            "æˆåŠŸç”Ÿæˆ": f"{actual_count} å¼ ",
            "æ€»è€—æ—¶": f"{total_time:.2f}s",
            "å¹³å‡é€Ÿåº¦": f"{avg_time:.2f}s/å¼ "
        })

        return (image_tensor, combined_text)

# æ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {"HeiHe001_BananaImageNode": BananaImageNode}
NODE_DISPLAY_NAME_MAPPINGS = {"HeiHe001_BananaImageNode": "Banana-API-2"}
